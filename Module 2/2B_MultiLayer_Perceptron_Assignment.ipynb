{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Assignment: Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptrons\n",
    "\n",
    "The simple logistic regression example we went over in the previous notebook is essentially a one-layer neural network, projecting straight from the input to the output predictions.\n",
    "While this can be effective for linearly separable data, occasionally a little more complexity is necessary.\n",
    "Neural networks with additional layers are typically able to learn more complex functions, leading to better performance.\n",
    "These additional layers (called \"hidden\" layers) transform the input into one or more intermediate representations before making a final prediction.\n",
    "\n",
    "In the logistic regression example, the way we performed the transformation was with a fully-connected layer, which consisted of a linear transform (matrix multiply plus a bias).\n",
    "A neural network consisting of multiple successive fully-connected layers is commonly called a Multi-Layer Perceptron (MLP). \n",
    "In the simple MLP below, a 4-d input is projected to a 5-d hidden representation, which is then projected to a single output that is used to make the final prediction.\n",
    "\n",
    "<img src=\"Figures/MLP.png\" width=\"300\"/>\n",
    "\n",
    "For the assignment, you will be building a MLP for MNIST.\n",
    "Mechanically, this is done very similary to our logistic regression example, but instead of going straight to a 10-d vector representing our output predictions, we might first transform to a 500-d vector with a \"hidden\" layer, then to the output of dimension 10.\n",
    "Before you do so, however, there's one more important thing to consider.\n",
    "\n",
    "### Nonlinearities\n",
    "\n",
    "We typically include nonlinearities between layers of a neural network.\n",
    "There's a number of reasons to do so.\n",
    "For one, without anything nonlinear between them, successive linear transforms (fully connected layers) collapse into a single linear transform, which means the model isn't any more expressive than a single layer.\n",
    "On the other hand, intermediate nonlinearities prevent this collapse, allowing neural networks to approximate more complex functions.\n",
    "\n",
    "There are a number of nonlinearities commonly used in neural networks, but one of the most popular is the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)):\n",
    "\n",
    "\\begin{align}\n",
    "x = \\max(0,x)\n",
    "\\end{align}\n",
    "\n",
    "There are a number of ways to implement this in PyTorch.\n",
    "We could do it with elementary PyTorch operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 0.6884,  0.8074,  0.8749],\n",
      "        [ 0.2012, -0.2841,  0.7612],\n",
      "        [-0.1938,  0.6472, -0.1155],\n",
      "        [ 0.9222,  0.9166, -0.7331],\n",
      "        [-0.9161, -0.1950, -0.5097]])\n",
      "x after ReLU with max: tensor([[0.6884, 0.8074, 0.8749],\n",
      "        [0.2012, 0.0000, 0.7612],\n",
      "        [0.0000, 0.6472, 0.0000],\n",
      "        [0.9222, 0.9166, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)*2 - 1\n",
    "x_relu_max = torch.max(torch.zeros_like(x),x)\n",
    "\n",
    "print(\"x: {}\".format(x))\n",
    "print(\"x after ReLU with max: {}\".format(x_relu_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, PyTorch also has the ReLU implemented, for example in `torch.nn.functional`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after ReLU with nn.functional: tensor([[0.6884, 0.8074, 0.8749],\n",
      "        [0.2012, 0.0000, 0.7612],\n",
      "        [0.0000, 0.6472, 0.0000],\n",
      "        [0.9222, 0.9166, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_relu_F = F.relu(x)\n",
    "\n",
    "print(\"x after ReLU with nn.functional: {}\".format(x_relu_F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Build a 2-layer MLP for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image (784 dimensions) ->  \n",
    "fully connected layer (500 hidden units) -> nonlinearity (ReLU) ->  \n",
    "fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Try building the model both with basic PyTorch operations, and then again with more object-oriented higher-level APIs. \n",
    "You should get similar results!\n",
    "\n",
    "\n",
    "*Some hints*:\n",
    "- Even as we add additional layers, we still only require a single optimizer to learn the parameters.\n",
    "Just make sure to pass all parameters to it!\n",
    "- As you'll calculate in the Short Answer, this MLP model has many more parameters than the logisitic regression example, which makes it more challenging to learn.\n",
    "To get the best performance, you may want to play with the learning rate and increase the number of training epochs.\n",
    "- Be careful using `torch.nn.CrossEntropyLoss()`. \n",
    "If you look at the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#crossentropyloss): you'll see that `torch.nn.CrossEntropyLoss()` combines the softmax operation with the cross-entropy.\n",
    "This means you need to pass in the logits (predictions pre-softmax) to this loss.\n",
    "Computing the softmax separately and feeding the result into `torch.nn.CrossEntropyLoss()` will significantly degrade your model's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c0e95690484da2a5d138ffbaac98aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: Remote end closed connection without response\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a424fe1629cc4ffc905fafaf335e41d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: Remote end closed connection without response\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95118c459cb941e49a18d2e285f0c176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: Remote end closed connection without response\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706756b14f8a472b83680c33276df259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: Remote end closed connection without response\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3649bef3520a4432a1457086d8e5d887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: Remote end closed connection without response\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to download dataset after several attempts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b6c4fe7328e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Download MNIST dataset with retries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b6c4fe7328e1>\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error downloading dataset: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for 5 seconds before retrying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to download dataset after several attempts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Transform to normalize the data and flatten the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to download dataset after several attempts"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Define a function to retry downloading the dataset\n",
    "def download_data():\n",
    "    for _ in range(5):  # Retry up to 5 times\n",
    "        try:\n",
    "            train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "            test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "            return train_dataset, test_dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading dataset: {e}\")\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "    raise RuntimeError(\"Failed to download dataset after several attempts\")\n",
    "\n",
    "# Transform to normalize the data and flatten the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "# Download MNIST dataset with retries\n",
    "train_dataset, test_dataset = download_data()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "class BasicMLP:\n",
    "    def __init__(self):\n",
    "        self.W1 = torch.randn(784, 500, requires_grad=True) * 0.01\n",
    "        self.b1 = torch.zeros(500, requires_grad=True)\n",
    "        self.W2 = torch.randn(500, 10, requires_grad=True) * 0.01\n",
    "        self.b2 = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = x @ self.W1 + self.b1\n",
    "        a1 = torch.relu(z1)\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        return z2  # We return the logits here\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "# Instantiate the model\n",
    "model = BasicMLP()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # Again, we return the logits\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Combines softmax and cross-entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Step {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "How many trainable parameters does your model have? \n",
    "How does this compare to the logisitic regression example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable Parameters Calculation:\n",
    "\n",
    "#### Model Architecture:\n",
    "1. **Input Layer to Hidden Layer (784 -> 500)**\n",
    "   - **Weights:** 784 input units * 500 hidden units = 392,000 parameters\n",
    "   - **Biases:** 500 hidden units = 500 parameters\n",
    "\n",
    "2. **Hidden Layer to Output Layer (500 -> 10)**\n",
    "   - **Weights:** 500 hidden units * 10 output units = 5,000 parameters\n",
    "   - **Biases:** 10 output units = 10 parameters\n",
    "\n",
    "#### Total Trainable Parameters:\n",
    "- **Total:** 392,000 (weights) + 500 (biases) + 5,000 (weights) + 10 (biases) = **397,510** parameters\n",
    "\n",
    "### Comparison to Logistic Regression Example:\n",
    "- **Logistic Regression Model (784 -> 10):**\n",
    "   - **Weights:** 784 input units * 10 output units = 7,840 parameters\n",
    "   - **Biases:** 10 output units = 10 parameters\n",
    "   - **Total:** 7,840 + 10 = **7,850** parameters\n",
    "\n",
    "### Summary:\n",
    "- Your 2-layer MLP model has **397,510** trainable parameters.\n",
    "- The logistic regression model has **7,850** trainable parameters.\n",
    "\n",
    "Thus, the MLP model has significantly more parameters than the logistic regression model, which makes it more complex and potentially more powerful, but also more challenging to train effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
